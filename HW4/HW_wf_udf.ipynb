{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E3Q9g_UyNxS6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .appName(\"Lesson_2\")\\\n",
    "    .config(\"spark.executor.instances\",2)\\\n",
    "    .config(\"spark.executor.memory\",'2g')\\\n",
    "    .config(\"spark.executor.cores\",1)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVGNGR7pN1KC"
   },
   "source": [
    "# Самостоятельная работа к уроку 4\n",
    "На уроке мы попробовали оконные и пользовательские функции. Теперь закрепим полученные знания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agigNChqOHnK"
   },
   "source": [
    "## Данные: [google drive: raw_sales.csv](https://drive.google.com/file/d/1G2N7Mnt4-Tqz4JdJxutGDMbJiOr32kZp/view?usp=sharing)\n",
    "\n",
    " Каждая строчка это продажа жилья, которая состоит из следующих полей (думаю описание не требуется):\n",
    "*   date of sale\n",
    "*   price\n",
    "*   property type\n",
    "*   number of bedrooms\n",
    "*   4digit postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datesold: timestamp (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- propertyType: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      "\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "|datesold           |postcode|price  |propertyType|bedrooms|\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "|2007-02-07 00:00:00|2607    |525000 |house       |4       |\n",
      "|2007-02-27 00:00:00|2906    |290000 |house       |3       |\n",
      "|2007-03-07 00:00:00|2905    |328000 |house       |3       |\n",
      "|2007-03-09 00:00:00|2905    |380000 |house       |4       |\n",
      "|2007-03-21 00:00:00|2906    |310000 |house       |3       |\n",
      "|2007-04-04 00:00:00|2905    |465000 |house       |4       |\n",
      "|2007-04-24 00:00:00|2607    |399000 |house       |3       |\n",
      "|2007-04-30 00:00:00|2606    |1530000|house       |4       |\n",
      "|2007-05-24 00:00:00|2902    |359000 |house       |3       |\n",
      "|2007-05-25 00:00:00|2906    |320000 |house       |3       |\n",
      "|2007-06-26 00:00:00|2902    |385000 |house       |3       |\n",
      "|2007-06-27 00:00:00|2906    |305000 |house       |3       |\n",
      "|2007-06-27 00:00:00|2612    |850000 |house       |4       |\n",
      "|2007-06-28 00:00:00|2904    |765000 |house       |4       |\n",
      "|2007-06-30 00:00:00|2615    |517000 |house       |4       |\n",
      "|2007-07-02 00:00:00|2914    |800000 |house       |5       |\n",
      "|2007-07-03 00:00:00|2906    |336000 |house       |3       |\n",
      "|2007-07-06 00:00:00|2615    |535000 |house       |5       |\n",
      "|2007-07-07 00:00:00|2602    |900000 |house       |4       |\n",
      "|2007-07-08 00:00:00|2600    |327000 |house       |1       |\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('raw_sales.csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xisyFowtQgx-"
   },
   "source": [
    "## Задание 1\n",
    "Добавьте к таблице следующие поля:\n",
    "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode)\n",
    "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\n",
    "*  Стоимость последнего проданного дома до текущего\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "|           datesold|postcode|  price|propertyType|bedrooms|       avg_before|\n",
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "|2007-07-08 00:00:00|    2600| 327000|       house|       1|             null|\n",
      "|2007-08-16 00:00:00|    2600| 790000|       house|       4|         327000.0|\n",
      "|2007-12-05 00:00:00|    2600| 825000|       house|       3|         558500.0|\n",
      "|2008-01-21 00:00:00|    2600| 315000|        unit|       1|647333.3333333334|\n",
      "|2008-04-24 00:00:00|    2600| 292500|       house|       1|         564250.0|\n",
      "|2008-05-30 00:00:00|    2600| 329000|        unit|       2|         509900.0|\n",
      "|2008-06-19 00:00:00|    2600| 765000|       house|       5|         479750.0|\n",
      "|2008-07-29 00:00:00|    2600| 927000|       house|       4|         520500.0|\n",
      "|2008-09-02 00:00:00|    2600|1380000|       house|       5|         571312.5|\n",
      "|2008-09-08 00:00:00|    2600| 740000|       house|       3|661166.6666666666|\n",
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "windSpec_before = Window()\\\n",
    "    .partitionBy('postcode')\\\n",
    "    .orderBy('datesold')\\\n",
    "    .rowsBetween(Window.currentRow - 11, Window.currentRow - 1)\n",
    "\n",
    "df.withColumn('avg_before', F.avg('price').over(windSpec_before)).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "|           datesold|postcode|  price|propertyType|bedrooms|       avg_before|\n",
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "|2007-07-08 00:00:00|    2600| 327000|       house|       1|706681.8181818182|\n",
      "|2007-08-16 00:00:00|    2600| 790000|       house|       4|692590.9090909091|\n",
      "|2007-12-05 00:00:00|    2600| 825000|       house|       3|703954.5454545454|\n",
      "|2008-01-21 00:00:00|    2600| 315000|        unit|       1|741681.8181818182|\n",
      "|2008-04-24 00:00:00|    2600| 292500|       house|       1|792818.1818181818|\n",
      "|2008-05-30 00:00:00|    2600| 329000|        unit|       2|859045.4545454546|\n",
      "|2008-06-19 00:00:00|    2600| 765000|       house|       5|816772.7272727273|\n",
      "|2008-07-29 00:00:00|    2600| 927000|       house|       4|775681.8181818182|\n",
      "|2008-09-02 00:00:00|    2600|1380000|       house|       5|754772.7272727273|\n",
      "|2008-09-08 00:00:00|    2600| 740000|       house|       3|739772.7272727273|\n",
      "+-------------------+--------+-------+------------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "windSpec_before = Window()\\\n",
    "    .partitionBy('postcode')\\\n",
    "    .orderBy('datesold')\\\n",
    "    .rowsBetween(Window.currentRow +1, Window.currentRow +11)\n",
    "\n",
    "df.withColumn('avg_before', F.avg('price').over(windSpec_before)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+------------+--------+----------+\n",
      "|           datesold|postcode|  price|propertyType|bedrooms|avg_before|\n",
      "+-------------------+--------+-------+------------+--------+----------+\n",
      "|2007-07-08 00:00:00|    2600| 327000|       house|       1|      null|\n",
      "|2007-08-16 00:00:00|    2600| 790000|       house|       4|    327000|\n",
      "|2007-12-05 00:00:00|    2600| 825000|       house|       3|    790000|\n",
      "|2008-01-21 00:00:00|    2600| 315000|        unit|       1|    825000|\n",
      "|2008-04-24 00:00:00|    2600| 292500|       house|       1|    315000|\n",
      "|2008-05-30 00:00:00|    2600| 329000|        unit|       2|    292500|\n",
      "|2008-06-19 00:00:00|    2600| 765000|       house|       5|    329000|\n",
      "|2008-07-29 00:00:00|    2600| 927000|       house|       4|    765000|\n",
      "|2008-09-02 00:00:00|    2600|1380000|       house|       5|    927000|\n",
      "|2008-09-08 00:00:00|    2600| 740000|       house|       3|   1380000|\n",
      "+-------------------+--------+-------+------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "windSpec_before = Window()\\\n",
    "    .partitionBy('postcode')\\\n",
    "    .orderBy('datesold')\\\n",
    "    .rowsBetween(Window.currentRow - 1, Window.currentRow - 1)\n",
    "\n",
    "df.withColumn('avg_before', F.sum('price').over(windSpec_before)).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvh2x6_8YU3F"
   },
   "source": [
    "## Задание 2\n",
    "В итоге у вас таблица с колонками (или нечто похожее):\n",
    "*   price\n",
    "*   Среднегодовая цена\n",
    "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode) (1 балл)\n",
    "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode) (1 балл)\n",
    "*  Стоимость последнего проданного дома до текущего ((1 балл)\n",
    "*  и др.\n",
    "\n",
    "Посчитайте кол-во уникальных значений в каждой строчке (unique(row)) (ипользуйте udf). Попробуйте сделать то же самое используя pandas udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSZTI9PAwQb"
   },
   "source": [
    "# Задание 3\n",
    "SQL like case when или if elif else\n",
    "\n",
    "Создайте колонку, в которой в которой будет отображаться \"+\", \"-\" или \"=\", если \"Средняя стомость 10 проданных домов до текущего в том же районе\" больше, меньше или равно \"Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\", соотвественно.\n",
    "\n",
    "Если одно из полей Null, запишите в эту колонку \"Нет данных\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+------------+--------+------+\n",
      "|           datesold|postcode|  price|propertyType|bedrooms|ifelse|\n",
      "+-------------------+--------+-------+------------+--------+------+\n",
      "|2007-02-07 00:00:00|    2607| 525000|       house|       4|     -|\n",
      "|2007-02-27 00:00:00|    2906| 290000|       house|       3|     -|\n",
      "|2007-03-07 00:00:00|    2905| 328000|       house|       3|     -|\n",
      "|2007-03-09 00:00:00|    2905| 380000|       house|       4|     -|\n",
      "|2007-03-21 00:00:00|    2906| 310000|       house|       3|     -|\n",
      "|2007-04-04 00:00:00|    2905| 465000|       house|       4|     -|\n",
      "|2007-04-24 00:00:00|    2607| 399000|       house|       3|     -|\n",
      "|2007-04-30 00:00:00|    2606|1530000|       house|       4|     -|\n",
      "|2007-05-24 00:00:00|    2902| 359000|       house|       3|     -|\n",
      "|2007-05-25 00:00:00|    2906| 320000|       house|       3|     -|\n",
      "|2007-06-26 00:00:00|    2902| 385000|       house|       3|     -|\n",
      "|2007-06-27 00:00:00|    2906| 305000|       house|       3|     -|\n",
      "|2007-06-27 00:00:00|    2612| 850000|       house|       4|     -|\n",
      "|2007-06-28 00:00:00|    2904| 765000|       house|       4|     -|\n",
      "|2007-06-30 00:00:00|    2615| 517000|       house|       4|     -|\n",
      "|2007-07-02 00:00:00|    2914| 800000|       house|       5|     -|\n",
      "|2007-07-03 00:00:00|    2906| 336000|       house|       3|     -|\n",
      "|2007-07-06 00:00:00|    2615| 535000|       house|       5|     -|\n",
      "|2007-07-07 00:00:00|    2602| 900000|       house|       4|     -|\n",
      "|2007-07-08 00:00:00|    2600| 327000|       house|       1|     -|\n",
      "+-------------------+--------+-------+------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ifelse', F.when(F.col('postcode').isNull() |  F.col('price').isNull(), F.lit('Нет данных')).when(F.col('postcode') > F.col('price'), F.lit('+')).when(F.col('postcode') < F.col('price'), '-').otherwise('=')).show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
